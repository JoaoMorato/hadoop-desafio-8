{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8a92d8-f5ba-4111-b4bc-9a428309330c",
   "metadata": {},
   "source": [
    "# Análise de Dados com PySpark e MySQL\n",
    "\n",
    "Este notebook demonstra como usar PySpark para ler um conjunto de arquivos de texto de mais de 500MB e inserir os dados linha a linha em uma tabela de banco de dados MySQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b834f38-06c8-48ae-97d0-886b3d6bb705",
   "metadata": {},
   "source": [
    "## Inicialização do Spark\n",
    "\n",
    "Vamos iniciar a sessão Spark e configurar a conexão com o MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53846d63-4587-41f5-99f7-6a50027aeb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.4.0-cp39-cp39-manylinux_2_17_x86_64.whl (19.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.4 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dfa34-935e-4f57-a9ea-12b1ca92e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import mysql.connector\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac254d3-bca2-40b8-bb03-71696d0c5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando Spark\n",
    "findspark.init(\"/usr/spark-3.5.1/\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"sparksubmit_test_app\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs:///user/hive/warehouse\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbff69-0bc4-47c6-b5f0-766e659973e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar um contexto de sessão do spark (cria um \"programa\")\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39077463-b81f-47c7-a5f2-cf0eab6e8210",
   "metadata": {},
   "source": [
    "## Conectando ao MySQL\n",
    "\n",
    "Vamos configurar a conexão com o banco de dados MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39bfdf-6a5a-40aa-9237-60e528f5362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão com o MySQL\n",
    "con = mysql.connector.connect(\n",
    "    host=\"10.5.0.5\",\n",
    "    port=3306,\n",
    "    user=\"hadoop\",\n",
    "    password=\"123456\",\n",
    "    database=\"db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar a tabela se não existir\n",
    "def create_table_if_not_exists():\n",
    "    cursor = con.cursor()\n",
    "    table_creation_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS text_data (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        texto TEXT\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(table_creation_query)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para inserir os dados no MySQL\n",
    "def insert_into_mysql(text):\n",
    "    cursor = con.cursor()\n",
    "    sql = \"INSERT INTO text_data (texto) values (%s)\"\n",
    "    valores = (text,)\n",
    "    cursor.execute(sql, valores)\n",
    "    cursor.close()\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler os textos do MySQL\n",
    "def read_from_mysql():\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute(\"SELECT texto FROM text_data\")\n",
    "    rows = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed441cb9-2769-464a-b23f-cba55826f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a tabela se não existir\n",
    "create_table_if_not_exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ae89b-db7d-4490-aaf6-20b35c471e02",
   "metadata": {},
   "source": [
    "## Leitura e Processamento dos Arquivos de Texto\n",
    "\n",
    "Vamos ler os arquivos de texto maiores que 500MB e processar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775ec77-ff7e-44da-b7d4-4faebc77839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para os arquivos de texto\n",
    "file_path = \"hdfs://spark-master:9000/datasets/*.txt\"\n",
    "\n",
    "# Leitura dos arquivos de texto\n",
    "text_files = spark.read.text(file_path)\n",
    "\n",
    "# Inserindo cada linha do arquivo no MySQL\n",
    "for row in text_files.collect():\n",
    "    # Pegando o texto da linha\n",
    "    text = row[0]\n",
    "    # Inserir no MySQL\n",
    "    insert_into_mysql(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260c088-e135-4673-a1b0-380f1a280c7a",
   "metadata": {},
   "source": [
    "## Contagem de Palavras\n",
    "\n",
    "Vamos contar as palavras nos arquivos de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c5389-b18d-46da-b47f-ecf9290073aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo os textos do MySQL\n",
    "texts = read_from_mysql()\n",
    "\n",
    "# Convertendo os textos em RDD\n",
    "texts_rdd = spark.sparkContext.parallelize([row[0] for row in texts])\n",
    "\n",
    "# Processamento das palavras\n",
    "words = texts_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Salvando os resultados no HDFS\n",
    "output_path = \"hdfs://spark-master:9000/datasets/word_count\"\n",
    "word_counts.saveAsTextFile(output_path)\n",
    "\n",
    "# Imprimindo os resultados\n",
    "print(f\"Total de palavras: {word_counts.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc47db-bd70-48e7-b6b8-ffa867254a0b",
   "metadata": {},
   "source": [
    "## Finalizando a Sessão Spark\n",
    "\n",
    "Vamos parar a sessão Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700add6-c910-4af2-a407-53f855cc1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
